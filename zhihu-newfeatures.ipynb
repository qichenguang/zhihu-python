{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# jieba pathon API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 搜索引擎模式\n",
    "seg_list = jieba.cut_for_search(\"小明硕士毕业于中国科学院计算所，后在日本京都大学深造\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict(seg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chilist = list(seg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chilist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 加载文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "from pysqlite2 import dbapi2 as sqlite\n",
    "import re\n",
    "import math\n",
    "import os\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 搜索引擎模式\n",
    "def separatewords(text):\n",
    "    # 搜索引擎模式\n",
    "    seg_list = jieba.cut_for_search(text)  \n",
    "    words = [s for s in list(seg_list) if len(s)>2 and len(s) < 20]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 遍历所有文件，提取标题和正本\n",
    "def getarticlewords(rootdir):\n",
    "    # 默认的 0 值，为了防止除 0 \n",
    "    mincount = 0.000000001\n",
    "    # 所有单词计数\n",
    "    allwords = {}\n",
    "    # 文章单词计数\n",
    "    articlewords = []\n",
    "    # 文章标题\n",
    "    articletitles = []\n",
    "    # 文章数组索引值\n",
    "    ec = 0\n",
    "    \n",
    "    # Loop over every file\n",
    "    i = 0\n",
    "    for parent,dirnames,filenames in os.walk(rootdir):\n",
    "        #输出文件信息\n",
    "        for filename in filenames:                        \n",
    "            #print \"parent is:\" + parent\n",
    "            #print type(parent)\n",
    "            #print \"filename is: \" + str(filename).decode('gbk', 'ignore')\n",
    "            #输出文件路径信息\n",
    "            #print \"the full name of the file is:\" + os.path.join(parent,filename) \n",
    "            file_class = parent.split(\"\\\\\")[1]\n",
    "            #print file_class\n",
    "            full_name = os.path.join(parent,filename) \n",
    "            i += 1\n",
    "            print str(i) + \" ,full_name is \" + str(full_name).decode('gbk', 'ignore')\n",
    "            # 读取文件\n",
    "            title,author,zhan_num,url,content = read_and_parse_file(full_name)\n",
    "            if title == None:\n",
    "                continue\n",
    "                \n",
    "            # Extract the words\n",
    "            txt = title + \" \" + content\n",
    "            words = separatewords(txt)\n",
    "            articlewords.append({})\n",
    "            # 文章标题\n",
    "            articletitles.append(title)\n",
    "\n",
    "            # Increase the counts for this word in allwords and in articlewords\n",
    "            for word in words:\n",
    "                allwords.setdefault(word,mincount)\n",
    "                # 该单词在所有文章中的总计数\n",
    "                allwords[word]+=1\n",
    "                articlewords[ec].setdefault(word,mincount)\n",
    "                # 该单词在该文章中的单词计数\n",
    "                articlewords[ec][word]+=1\n",
    "            #\n",
    "            ec+=1\n",
    "                  \n",
    "    # 返回 单词频次，文章中单词频次，文章标题\n",
    "    return allwords,articlewords,articletitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_and_parse_file(file_path):\n",
    "    # 去掉标点符号\n",
    "    delset = string.punctuation\n",
    "    try:\n",
    "        if os.path.exists(file_path):\n",
    "            f = open(file_path, \"r\")\n",
    "            lines = f.readlines()\n",
    "            f.close()\n",
    "            if len(lines) < 5:\n",
    "                return None,None,0,None,None\n",
    "            title = lines[0].strip()\n",
    "            fields = lines[2].split(':')\n",
    "            zhan_num = int(fields[2].strip())\n",
    "            author = fields[1].strip().split(' ')[0].strip()\n",
    "            content = ''.join(lines[5:-2])\n",
    "            url = lines[-1].split(' ')[1].strip()\n",
    "            #\n",
    "            title = title.translate(None,delset)\n",
    "            author = author.translate(None,delset)\n",
    "            content = content.translate(None,delset)\n",
    "            return title,author,zhan_num,url,content\n",
    "    except IOError:\n",
    "        error_file_path = file_path\n",
    "        print \"error_file_path:\" , error_file_path\n",
    "    \n",
    "    # 默认\n",
    "    return None,None,0,None,None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 转换成矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 【单词频次 , 文章中单词频次】\n",
    "def makematrix(allw,articlew):\n",
    "    # 默认的 0 值，为了防止除 0 \n",
    "    mincount = 0.000000001\n",
    "\n",
    "    wordvec = []\n",
    "\n",
    "    # Only take words that are common but not too common\n",
    "    # 过滤掉单词太频繁和太不频繁的单词\n",
    "    for w,c in allw.items():\n",
    "        # 至少出现过一次并且最多在 60% 的文章中出现过，否则太频繁了\n",
    "        if c>3 and c<len(articlew)*0.6:\n",
    "            wordvec.append(w) \n",
    "\n",
    "    # Create the word matrix\n",
    "    #l1=[[(word in f and f[word] or 0) for word in wordvec] for f in articlew]\n",
    "    # 矩阵\n",
    "    # 行 ： 文章 \n",
    "    # 列 ： 单词\n",
    "    # 值 ： 单词在文章中出现的次数\n",
    "    l1=[[(word in f and f[word] or mincount) for word in wordvec] for f in articlew]\n",
    "\n",
    "    # 返回 文章单词矩阵, 单词列表(矩阵的列)\n",
    "    return l1,wordvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rootdir = 'E:/project/pychram/zhihu/zhihu-python/text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 单词频次，文章中单词频次，文章标题\n",
    "allwords,articlewords,articletitles = getarticlewords(rootdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 文章单词矩阵, 单词列表(矩阵的列)\n",
    "wordmatrix,wordvec = makematrix(allwords,articlewords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordvec[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "articletitles[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordmatrix[1][0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(allwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 因式分解函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "\n",
    "# 计算两个矩阵的距离差平方累加值\n",
    "def difcost(a,b):\n",
    "    dif=0\n",
    "    for i in range(shape(a)[0]):\n",
    "        for j in range(shape(a)[1]):\n",
    "            # Euclidean Distance\n",
    "            dif+=pow(a[i,j]-b[i,j],2)\n",
    "    return dif\n",
    "\n",
    "# 分解矩阵为 权重矩阵 * 特征矩阵\n",
    "# 【输入矩阵,需要输出的特征数量,迭代次数】\n",
    "def factorize(v,pc=10,iter=50):\n",
    "    ic=shape(v)[0]\n",
    "    fc=shape(v)[1]\n",
    "\n",
    "    # Initialize the weight and feature matrices with random values\n",
    "    w=matrix([[random.random() for j in range(pc)] for i in range(ic)])\n",
    "    h=matrix([[random.random() for i in range(fc)] for i in range(pc)])\n",
    "\n",
    "    # Perform operation a maximum of iter times\n",
    "    for i in range(iter):\n",
    "        wh=w*h\n",
    "\n",
    "        # Calculate the current difference\n",
    "        cost=difcost(v,wh)\n",
    "\n",
    "        if i%10==0: print cost\n",
    "\n",
    "        # Terminate if the matrix has been fully factorized\n",
    "        if cost==0: break\n",
    "\n",
    "        # Update feature matrix\n",
    "        hn=(transpose(w)*v)\n",
    "        hd=(transpose(w)*w*h)\n",
    "\n",
    "        h=matrix(array(h)*array(hn)/array(hd))\n",
    "\n",
    "        # Update weights matrix\n",
    "        wn=(v*transpose(h))\n",
    "        wd=(w*h*transpose(h))\n",
    "\n",
    "        w=matrix(array(w)*array(wn)/array(wd))  \n",
    "\n",
    "    return w,h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = matrix(wordmatrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights,feat = factorize(v,pc=10,iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 遍历特征矩阵,显示每一行特征的 前 6 个特征次数最多的单词\n",
    "# 遍历权重矩阵,显示该特征(列) 的行(文章) 的权重值最高的 3 篇文章标题\n",
    "def showfeatures(w,h,titles,wordvec,out='features.txt'): \n",
    "    outfile = file(out,'w')  \n",
    "    # pc 行：特征\n",
    "    # wc 列：单词\n",
    "    # [pc,wc] : 单词在特征中出现的次数\n",
    "    pc,wc = shape(h)\n",
    "    # 行： 标题文章\n",
    "    toppatterns = [[] for i in range(len(titles))]\n",
    "    patternnames = []\n",
    "\n",
    "    # Loop over all the features\n",
    "    # 遍历每一行：特征\n",
    "    pos = 0\n",
    "    for i in range(pc):\n",
    "        pos += 1\n",
    "        slist = []\n",
    "        # Create a list of words and their weights\n",
    "        # 遍历每一列 ：单词\n",
    "        for j in range(wc):\n",
    "            # [pc,wc] : 单词在特征中出现的次数 ，单词的内容\n",
    "            slist.append((h[i,j],wordvec[j]))\n",
    "        # Reverse sort the word list\n",
    "        # 按 特征次数排序，降序\n",
    "        slist.sort()\n",
    "        slist.reverse()\n",
    "\n",
    "        # Print the first six elements\n",
    "        # 这一特征行中 前 6 位的 单词的内容数组\n",
    "        n = [s[1] for s in slist[0:6]]\n",
    "        # 这一特征行中 显示 前 6 个单词内容\n",
    "        n_str_list = [ item.encode('UTF-8','ignore') for item in n]\n",
    "        n_str = ','.join(n_str_list)\n",
    "        outfile.write(n_str + '\\n')\n",
    "        print str(pos) + \" : \" + n_str + '\\n'\n",
    "        \n",
    "        # 将 前 6 位的 单词的内容数组 放入模式名称数组中\n",
    "        # 行 ： 特征\n",
    "        # 列 ： 前 6 位的 单词\n",
    "        patternnames.append(n)\n",
    "\n",
    "        # Create a list of articles for this feature\n",
    "        \n",
    "        flist=[]\n",
    "        # 遍历每一篇文章标题\n",
    "        # 行：文章\n",
    "        for j in range(len(titles)):\n",
    "            # Add the article with its weight\n",
    "            # 列 ： 特征\n",
    "            # [文章,特征] ： 特征在文章中的权重，文章标题\n",
    "            flist.append((w[j,i],titles[j]))\n",
    "            # 行：文章 \n",
    "            # 列：(特征权重,特征矩阵中的特征索引-行索引,文章标题)\n",
    "            toppatterns[j].append((w[j,i],i,titles[j]))\n",
    "\n",
    "        # Reverse sort the list\n",
    "        # 按 特征在文章中的权重 排序，倒序\n",
    "        flist.sort()\n",
    "        flist.reverse()\n",
    "\n",
    "        # Show the top 3 articles\n",
    "        # 这一行特征中，该特征权重最大的 3 篇文章\n",
    "        for f in flist[0:3]:\n",
    "            # 特征在文章中的权重,文章标题\n",
    "            # 显示需要对字符串解码为 GBK\n",
    "            print f[0], f[1].decode('gbk','ignore') + '\\n'\n",
    "            \n",
    "            n_str = str(f[0]) + \", \" +  (f[1].decode('gbk','ignore')) + '\\n'\n",
    "            #print n_str\n",
    "            \n",
    "            # 写入文件需要对字符串加码为 UTF-8\n",
    "            outfile.write(n_str.encode('UTF-8','ignore') + '\\n')\n",
    "            \n",
    "        outfile.write('\\n')\n",
    "\n",
    "    outfile.close()\n",
    "    # Return the pattern names for later use\n",
    "    # 返回 \n",
    "    # toppatterns ： \n",
    "            # 行：文章 \n",
    "            # 列：(特征权重,特征索引,文章标题)\n",
    "    # patternnames\n",
    "            # 行 ： 特征\n",
    "            # 列 ： 前 6 位的 单词\n",
    "    return toppatterns,patternnames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "topp,pn = showfeatures(weights,feat,articletitles,wordvec)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 显示文章及权重最大的特征的前6个单词\n",
    "# 【# titles: 文章标题\n",
    "    # toppatterns ： \n",
    "            # 行：文章 \n",
    "            # 列：(特征权重,特征矩阵中的特征索引-行索引,文章标题)\n",
    "    # patternnames\n",
    "            # 行 ： 特征\n",
    "            # 列 ： 前 6 位的 单词】\n",
    "def showarticles(titles,toppatterns,patternnames,out='articles.txt'):\n",
    "    outfile = file(out,'w')  \n",
    "\n",
    "    # Loop over all the articles\n",
    "    # 遍历每一篇文章\n",
    "    pos = 0\n",
    "    for j in range(len(titles)):\n",
    "        pos += 1\n",
    "        # 打印这篇文章的标题\n",
    "        title_str = str(pos) + \" : \" + titles[j].decode('gbk','ignore') + '\\n'\n",
    "        print title_str\n",
    "        #\n",
    "        outfile.write(title_str.encode('UTF-8','ignore'))\n",
    "\n",
    "        # Get the top features for this article and\n",
    "        # reverse sort them\n",
    "        # 这篇文章的特征权重排序后，降序\n",
    "        toppatterns[j].sort()\n",
    "        toppatterns[j].reverse()\n",
    "\n",
    "        # Print the top three patterns\n",
    "        # 显示这篇文章的最大权重的前 3 个特征\n",
    "        # 遍历这篇文章的前3个特征\n",
    "        for i in range(3):\n",
    "            # 显示该特征在该文章中的权重\n",
    "            # 显示该特征在给文章中的前 6 位的特征单词\n",
    "            fea_str = str(toppatterns[j][i][0]) + \" : \" + ','.join(patternnames[toppatterns[j][i][1]]) + '\\n'\n",
    "            print fea_str\n",
    "            #\n",
    "            outfile.write(fea_str.encode('UTF-8','ignore') )\n",
    "        outfile.write('\\n')\n",
    "\n",
    "    outfile.close()\n",
    "\n",
    "showarticles(articletitles,topp,pn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
